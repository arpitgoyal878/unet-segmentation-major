{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arxiv Link: <a href=\"https://arxiv.org/abs/1505.04597\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>UNet is a fully convolutional network(FCN) that does image segmentation. Its goal is to predict each pixel's class.</li>\n",
    " \n",
    "<li>UNet is built upon the FCN and modified in a way that it yields better segmentation in medical imaging.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Architecture\n",
    "\n",
    "<img src=\"images/u-net-architecture.png\"/>\n",
    "\n",
    "<h3>UNet Architecture has 3 parts:</h3>\n",
    "<ol>\n",
    "    <li>The Contracting/Downsampling Path</li>\n",
    "    <li>Bottleneck</li>\n",
    "    <li>The Expanding/Upsampling Path</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Downsampling Path: </h3> \n",
    "<ol>\n",
    "    <li>It consists of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling.</li> \n",
    "    <li>At each downsampling step we double the number of feature channels.</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Upsampling Path: </h3> \n",
    "<ol>\n",
    "     <li> Every  step  in  the  expansive  path  consists  of  an  upsampling  of  the feature map followed by a 2x2 convolution (“up-convolution”), a concatenation with the correspondingly feature  map  from  the  downsampling  path,  and  two  3x3  convolutions,  each  followed by a ReLU.</li>\n",
    "</ol>\n",
    "\n",
    "<h3> Skip Connection: </h3>\n",
    "The skip connection from the downsampling path are concatenated with feature map during upsampling path. These skip connection provide local information to global information while upsampling.\n",
    "\n",
    "<h3> Final Layer: </h3>\n",
    "At the final layer a 1x1 convolution is used to map each feature vector to the desired number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Advantages\n",
    "<h3> Advantages: </h3>\n",
    "<ol>\n",
    "    <li>The UNet combines the location information from the downsampling path to finally obtain a general information combining localisation and context, which is necessary to predict a good segmentation map.</li>\n",
    "    <li>No Dense layer is used, so image sizes can be used.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dataset\n",
    "Link: <a href=\"https://www.kaggle.com/c/data-science-bowl-2018\">Data Science Bowl 2018</a>\n",
    "Find the nuclei in divergent images to advance medical discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpywt\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pywt\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "train_path = \"dataset/stage1_train/\"\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "## Training Ids\n",
    "train_ids = next(os.walk(train_path))[1]\n",
    "# train_ids = [dir for dir in next(os.walk(train_path))[1]]\n",
    "## Validation Data Size\n",
    "val_data_size = 10\n",
    "\n",
    "valid_ids = train_ids[:val_data_size]\n",
    "train_ids = train_ids[val_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DataGen(train_ids, train_path, batch_size=batch_size, image_size=image_size)\n",
    "x, y = gen.__getitem__(0)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "r = random.randint(0, len(x)-1)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "# ax = fig.add_subplot(1, 2, 1)\n",
    "# ax.imshow(x[r])\n",
    "# ax = fig.add_subplot(1, 2, 2)\n",
    "# ax.imshow(np.reshape(y[r], (image_size, image_size)), cmap=\"gray\")\n",
    "# Generate a random index\n",
    "r = random.randint(0, len(x)-1)\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Plot the original image\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "# Since the image has been transformed by wavelet, you may need to reshape it\n",
    "# For example, if LL is the transformed image, you can plot it like this:\n",
    "# ax.imshow(x[r], cmap='gray')  # Assuming the transformed image is grayscale\n",
    "# Assuming the transformed image has a shape of (128, 64, 3) after wavelet transformation\n",
    "ax.imshow(x[r][:, :, 0], cmap='gray')  # Display the first channel of the RGB image as grayscale\n",
    "\n",
    "# If LL is a multi-channel image, you need to select one channel for display\n",
    "\n",
    "# Plot the corresponding mask\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(y[r], (image_size, image_size)), cmap=\"gray\")  # Assuming y contains masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Convolutional Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    us = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    concat = keras.layers.Concatenate()([us, skip])\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet():\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "    inputs = keras.layers.Input((image_size, image_size, 3))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f[0]) #128 -> 64\n",
    "    c2, p2 = down_block(p1, f[1]) #64 -> 32\n",
    "    c3, p3 = down_block(p2, f[2]) #32 -> 16\n",
    "    c4, p4 = down_block(p3, f[3]) #16->8\n",
    "    \n",
    "    bn = bottleneck(p4, f[4])\n",
    "    \n",
    "    u1 = up_block(bn, c4, f[3]) #8 -> 16\n",
    "    u2 = up_block(u1, c3, f[2]) #16 -> 32\n",
    "    u3 = up_block(u2, c2, f[1]) #32 -> 64\n",
    "    u4 = up_block(u3, c1, f[0]) #64 -> 128\n",
    "    \n",
    "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
    "valid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
    "\n",
    "train_steps = len(train_ids)//batch_size\n",
    "valid_steps = len(valid_ids)//batch_size\n",
    "\n",
    "model.fit(train_gen, validation_data=valid_gen, steps_per_epoch=int(train_steps), validation_steps=int(valid_steps), \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the Weights\n",
    "model.save_weights(\"UNet.weights.h5\")\n",
    "\n",
    "## Dataset for prediction\n",
    "x, y = valid_gen.__getitem__(1)\n",
    "result = model.predict(x)\n",
    "\n",
    "result = result > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[0]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[0]*255, (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[1]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[1]*255, (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
